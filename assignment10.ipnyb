{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"assignment10.ipnyb","provenance":[{"file_id":"1bHNhafqHlIA3AaK0c6fAvfKFQBlNBnod","timestamp":1591081918804},{"file_id":"1Mr5v5OPayPbxpgFqqP9RAOAlAT6AeP9t","timestamp":1590465551420},{"file_id":"1ZOY7WhtpuImqOCq6xH9xb3SKjZpWsjDg","timestamp":1590042024344},{"file_id":"1lHsUCT-169HWJX_gH3AHVWMCPf8_fgp6","timestamp":1589267672437},{"file_id":"1kxLGScNJFdPu0TJjKnYjdBghuA8RpayJ","timestamp":1588227282478},{"file_id":"14sk42Vw2kXkbmmU_DRavxSB79umcYmY-","timestamp":1587433034302}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP/kYN9wOQ4Z+GMOlM9gx9j"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Eh-H3h7uYq8L","colab_type":"text"},"source":["Multi-label classification using neural networks\n","---"]},{"cell_type":"markdown","metadata":{"id":"TJv1FZVQW5RH","colab_type":"text"},"source":["1. input data"]},{"cell_type":"code","metadata":{"id":"ZPDm2rL0I6GP","colab_type":"code","outputId":"3d041017-daf2-4e13-f227-c6552c72d2ff","executionInfo":{"status":"ok","timestamp":1591088297234,"user_tz":-540,"elapsed":10370,"user":{"displayName":"조성민","photoUrl":"","userId":"00315417681750132645"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","file_data   = \"mnist.csv\"\n","handle_file = open(file_data, \"r\")\n","data        = handle_file.readlines()\n","handle_file.close()\n","\n","size_row    = 28    # height of the image\n","size_col    = 28    # width of the image\n","\n","num_image   = len(data)\n","count       = 0     # count for the number of images\n","print(num_image)    # num_image is 10000.\n","\n","\n","#\n","# normalize the values of the input data to be [0, 1]\n","#\n","def normalize(data):\n","\n","    data_normalized = (data - min(data)) / (max(data) - min(data))\n","\n","    return(data_normalized)\n","\n","#\n","# example of distance function between two vectors x and y\n","#\n","def distance(x, y):\n","\n","    d = (x - y) ** 2\n","    s = np.sum(d)\n","    # r = np.sqrt(s)\n","\n","    return(s)\n","\n","#\n","# make a matrix each column of which represents an images in a vector form\n","#\n","list_image  = np.empty((size_row * size_col, num_image), dtype=float)\n","list_label  = np.empty(num_image, dtype=int)\n","\n","for line in data:\n","\n","    line_data   = line.split(',')\n","    label       = line_data[0]\n","    im_vector   = np.asfarray(line_data[1:])\n","    im_vector   = normalize(im_vector)\n","\n","    list_label[count]       = label\n","    list_image[:, count]    = im_vector\n","\n","    count += 1\n","\n","#\n","# plot first 150 images out of 10,000 with their labels\n","#\n","f1 = plt.figure(1)\n","\n","for i in range(150):\n","\n","    label       = list_label[i]\n","    im_vector   = list_image[:, i]\n","    im_matrix   = im_vector.reshape((size_row, size_col))\n","\n","    plt.subplot(10, 15, i+1)\n","    plt.title(label)\n","    plt.imshow(im_matrix, cmap='Greys', interpolation='None')\n","\n","    frame   = plt.gca()\n","    frame.axes.get_xaxis().set_visible(False)\n","    frame.axes.get_yaxis().set_visible(False)\n","\n","\n","#plt.show()\n","\n","#\n","# plot the average image of all the images for each digit\n","#\n","f2 = plt.figure(2)\n","\n","im_average  = np.zeros((size_row * size_col, 10), dtype=float)\n","im_count    = np.zeros(10, dtype=int)\n","\n","for i in range(num_image):\n","\n","    im_average[:, list_label[i]] += list_image[:, i]\n","    im_count[list_label[i]] += 1\n","\n","for i in range(10):\n","\n","    im_average[:, i] /= im_count[i]\n","\n","    plt.subplot(2, 5, i+1)\n","    plt.title(i)\n","    plt.imshow(im_average[:,i].reshape((size_row, size_col)), cmap='Greys', interpolation='None')\n","\n","    frame   = plt.gca()\n","    frame.axes.get_xaxis().set_visible(False)\n","    frame.axes.get_yaxis().set_visible(False)\n","\n","plt.show()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HPo20R-uSEYE","colab_type":"text"},"source":["- load the data file ('mnist.csv')\n","- each row contains label is digit $l \\in [0, 9]$ and 28 by 28 image pixel values $x \\in R^{784}$"]},{"cell_type":"code","metadata":{"id":"cv63DD8gSCT2","colab_type":"code","colab":{}},"source":["print(list_image.shape)\n","\n","# list_image = list_image.transpose()\n","# list_label = list_label.transpose()\n","\n","# # convert label_to_one_hot\n","print(list_label.shape)  # 10000, \n","num = np.unique(list_label, axis=0)\n","num = num.shape[0]\n","list_label = np.eye(num)[list_label] # \n","list_label = list_label.transpose()  # 10, 10000\n","\n","train_image = list_image[:, :1000]\n","train_label = list_label[:, :1000]\n","test_image = list_image[:, 1000:]\n","test_label = list_label[:, 1000:]\n","\n","print(train_image.shape)  # (784, 1000)\n","print(train_label.shape)  # (10, 1000)\n","\n","print(test_image.shape)   # (784, 9000)\n","print(test_label.shape)   # (10, 9000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QI8W7pt2T3hs","colab_type":"text"},"source":["- consider first 1000 images for training and rest 9000 images for testing.\n","- normalized the image data whose ranges from 0 to 1.\n","- convert label to one-hot encoding label"]},{"cell_type":"markdown","metadata":{"id":"RGKX-MJ6Y4Kj","colab_type":"text"},"source":["---\n","2. neural network architecture\n"]},{"cell_type":"code","metadata":{"id":"mgF2ZBUKYXai","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def network_forward(input_x, first_theta, second_theta, third_theta, first_bias, second_bias, third_bias):\n","\n","  # input shape is 784, 1\n","  # input shape is [1, 784] \n","  # if input_x.dim\n","\n","  a_1 = input_x\n","\n","  y_ = np.matmul(first_theta, input_x) + first_bias # [1, 196]\n","  # print(\"y_:\", y_.shape)\n","  y = 1 / (1 + np.exp(-1 * y_))  # sigmoid\n","  a_2 = y\n","  \n","\n","  z_ = np.matmul(second_theta, y) + second_bias\n","  # print(\"z_:\", z_.shape)\n","  z = 1 / (1 + np.exp(-1 * z_))\n","  a_3 = z\n","\n","  h_ = np.matmul(third_theta, z) + third_bias\n","  #print(\"h_:\", h_.shape)\n","  h = 1 / (1 + np.exp(-1 * h_))\n","\n","  a_4 = h\n","  return a_1, a_2, a_3, a_4\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m1nUcJUqcDnE","colab_type":"text"},"source":["- make network architecture and forwarding."]},{"cell_type":"code","metadata":{"id":"V3fFRzJyg7nZ","colab_type":"code","colab":{}},"source":["sample_img = train_image[:, :1]\n","# sample_label = train_label[:, :]\n","\n","print(sample_img.shape)\n","# print(sample_label.shape)\n","\n","first_theta = np.random.normal(loc=0.0, scale=1.0, size=(196, 784))\n","second_theta = np.random.normal(loc=0.0, scale=1.0, size=(49, 196))\n","third_theta = np.random.normal(loc=0.0, scale=1.0, size=(10, 49))\n","\n","first_bias = np.random.normal(loc=0.0, scale=1.0, size=(196, 1))\n","second_bias = np.random.normal(loc=0.0, scale=1.0, size=(49, 1))\n","third_bias = np.random.normal(loc=0.0, scale=1.0, size=(10, 1))\n","\n","a_1, a_2, a_3, a_4 = network_forward(sample_img, first_theta, second_theta, third_theta, first_bias, second_bias, third_bias)\n","print(a_1.shape)\n","print(a_2.shape)\n","print(a_3.shape)\n","print(a_4.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pHZNp1nZNLe_","colab_type":"text"},"source":["- testing network forwarding"]},{"cell_type":"code","metadata":{"id":"fHmzfZXWjQzS","colab_type":"code","colab":{}},"source":["def network_backward(a_1, a_2, a_3, a_4, \n","                     x, y, \n","                     first_theta, second_theta, third_theta, \n","                     first_bias, second_bias, third_bias, \n","                     gradient_1, gradient_2, gradient_3, \n","                     learning_rate=1e-3,\n","                     lambda_=5e-4):\n","  \n","  delta_4 = (a_4 - y)                       # [10, 1]\n","  # print(\"delta_4.shape : \", delta_4.shape)\n","\n","  # [10, 49] --> [49, 10] dot [10, 1] --> \n","  # print(third_theta.shape)\n","  delta_3 = np.matmul(third_theta.transpose(), delta_4) * a_3 * (1-a_3)\n","  # print(\"a_3.shape : \", a_3.shape)\n","  # print(\"delta_3.shape : \", delta_3.shape)  # [49, 1]\n","\n","  delta_2 = np.matmul(second_theta.transpose(), delta_3) * a_2 * (1-a_2)\n","  # print(\"delta_2.shape : \", delta_2.shape)  # [196, 1]\n","\n","  # print(\"a_1.shape : \", a_1.shape)\n","\n","  new_gradient_1 = delta_2 @ a_1.transpose()  # [196, 1], [1, 784]  # 196, 784\n","  # print(new_gradient_1.shape)\n","  new_gradient_2 = delta_3 @ a_2.transpose()\n","  # print(new_gradient_2.shape)\n","  new_gradient_3 = delta_4 @ a_3.transpose()\n","  # print(new_gradient_3.shape)\n","   \n","  new_first_theta = first_theta - learning_rate * new_gradient_1 + first_theta * lambda_\n","  new_first_bias = first_bias - learning_rate * delta_2\n","  # print('new_first_theta.shape :', new_first_theta.shape)\n","\n","  new_second_theta = second_theta - learning_rate * new_gradient_2 + second_theta * lambda_\n","  new_second_bias = second_bias - learning_rate * delta_3\n","  #print('second_theta.shape :', new_second_theta.shape)\n","\n","  new_third_theta = third_theta - learning_rate * new_gradient_3 + third_theta * lambda_\n","  new_third_bias = third_bias - learning_rate * delta_4\n","  #print('third_theta.shape :', new_third_theta.shape)\n","  #return new_gradient_1\n","\n","  return new_first_theta, new_second_theta, new_third_theta, new_first_bias, new_second_bias, new_third_bias\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"48s40L-UvLbw","colab_type":"text"},"source":["- make back propagation for nn by myself.\n","- $new gradient^{(l + 1)} = gradient^{(l)} + \\delta_{j}^{(l+1)} * (a_{j}^{(l)})^{T}$"]},{"cell_type":"code","metadata":{"id":"mSTb3Oup_21M","colab_type":"code","colab":{}},"source":["# sample_img = train_image[:, :1]\n","# sample_label = train_label[:, :1]\n","\n","# print(sample_img.shape)\n","# print(sample_label.shape)\n","\n","# th1 = np.random.normal(loc=0.0, scale=1.0, size=(196, 784))\n","# th2 = np.random.normal(loc=0.0, scale=1.0, size=(49, 196))\n","# th3 = np.random.normal(loc=0.0, scale=1.0, size=(10, 49))\n","\n","# bias1 = np.random.normal(loc=0.0, scale=1.0, size=(196, 1))\n","# bias2 = np.random.normal(loc=0.0, scale=1.0, size=(49, 1))\n","# bias3 = np.random.normal(loc=0.0, scale=1.0, size=(10, 1))\n","\n","# a_1, a_2, a_3, a_4 = network_forward(sample_img, first_theta, second_theta, third_theta, first_bias, second_bias, third_bias)\n","# print(a_1.shape)\n","# print(a_2.shape)\n","# print(a_3.shape)\n","# print(a_4.shape)\n","\n","# gradient_1 = gradient_2 = gradient_3 = 0\n","\n","# th1, th2, th3 , bias1, bias2, bias3 = network_backward(a_1, a_2, a_3, a_4,\n","#                                                        sample_img, sample_label,\n","#                                                        th1, th2, th3, bias1, bias2, bias3,\n","#                                                        gradient_1, gradient_2, gradient_3)\n","# print(th1.shape)\n","# print(th2.shape)\n","# print(th3.shape)\n","# print(bias1.shape)\n","# print(bias2.shape)\n","# print(bias3.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wzw8SIxy1NFl","colab_type":"code","colab":{}},"source":["def criterion(a_4, y, th1, th2, th3, bias1, bias2, bias3, lambda_=5e-4):\n","  # a_4 - shape : [10, 1]\n","  # y - shape : [10, 1] \n","  # print(\"a_4's shape : \", a_4.shape)\n","  # print(\"y's shape : \", y.shape)\n","\n","  a_4 = a_4.squeeze()\n","  y = y.squeeze()\n","  \n","  l2_term = 0 \n","  l2_term += np.mean(np.square(th1)) \n","  l2_term += np.mean(np.square(th2))\n","  l2_term += np.mean(np.square(th3))  \n","  l2_term += np.mean(np.square(bias1)) \n","  l2_term += np.mean(np.square(bias2))\n","  l2_term += np.mean(np.square(bias3))  \n","\n","  if l2_term == 0:\n","    loss = np.mean(-1 * (y * np.log(a_4) + (1-y) * np.log(1-a_4)))\n","    return loss\n","    \n","  l2_term *= lambda_\n","  l2_term /= 2\n","\n","  loss = np.mean(-1 * (y * np.log(a_4) + (1-y) * np.log(1-a_4))) + l2_term\n","  return loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzphviVnO23t","colab_type":"text"},"source":["- make binary cross entropy loss for multi-label classification."]},{"cell_type":"code","metadata":{"id":"2ewJMfRcPaYe","colab_type":"code","colab":{}},"source":["def is_same(pred, gt_label):\n","  \"\"\"\n","  pred: [10, 1]\n","  gt_label : [10, 1]\n","  \"\"\"\n","  pred = np.argmax(a_4, axis=0)\n","  # print(pred)\n","  gt_label = np.argmax(label, axis=0)\n","\n","  # print(\"pred_label : \", pred)\n","  # print(\"gt_label : \", gt_label)\n","\n","  return int(pred==gt_label)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pYJ075rhy4ni","colab_type":"text"},"source":["--- \n","3. training"]},{"cell_type":"code","metadata":{"id":"w8p9inSpvOWp","colab_type":"code","colab":{}},"source":["lambda_ = 5e-4\n","\n","# training\n","th1 = np.random.normal(loc=0.0, scale=1.0, size=(196, 784))\n","th2 = np.random.normal(loc=0.0, scale=1.0, size=(49, 196))\n","th3 = np.random.normal(loc=0.0, scale=1.0, size=(10, 49))\n","\n","bias1 = np.random.normal(loc=0.0, scale=1.0, size=(196, 1))\n","bias2 = np.random.normal(loc=0.0, scale=1.0, size=(49, 1))\n","bias3 = np.random.normal(loc=0.0, scale=1.0, size=(10, 1))\n","\n","loss_list_for_train = []\n","acc_list_for_train = []\n","\n","loss_list_for_test = []\n","acc_list_for_test = []\n","\n","# define hyperparameter for multi-label classification\n","\n","epochs = 20\n","learning_rate = 1e-2\n","decay_step = 10\n","\n","for epoch in range(epochs):\n","  print('\\ntraining for epoch [{}]'.format(epoch))\n","  match_cnt = 0\n","  sum_loss = 0 \n","\n","  if epoch != 0 and epoch % decay_step == 0:\n","    learning_rate *= 0.1\n","\n","  # training \n","  for i in range(1000):\n","    # print(i)\n","\n","    if i == 999:\n","      acc = match_cnt / (i + 1)\n","      acc *= 100\n","      acc_list_for_train.append(acc)\n","      print(\"training_acc : {:.4f}\".format(acc))\n","\n","      mean_loss = sum_loss/(i + 1)\n","      loss_list_for_train.append(mean_loss)\n","\n","\n","    image = train_image[:, i]\n","    label = train_label[:, i]\n","\n","    image = np.expand_dims(image, axis=1) # 784 --> 784, 1\n","    label = np.expand_dims(label, axis=1) # 10 --> 10, 1\n","\n","    a_1, a_2, a_3, a_4 = network_forward(image, th1, th2, th3, bias1, bias2, bias3)\n","\n","    match_cnt += is_same(a_4, label)\n","    loss = criterion(a_4, label, th1, th2, th3, bias1, bias2, bias3, lambda_=lambda_)\n","    # loss_list_for_train.append(loss)\n","\n","    sum_loss += loss \n","\n","    gradient_1 = gradient_2 = gradient_3 = 0\n","    th1, th2, th3 , bias1, bias2, bias3 = network_backward(a_1, a_2, a_3, a_4,\n","                                                           image, label,\n","                                                           th1, th2, th3, bias1, bias2, bias3,\n","                                                           gradient_1, gradient_2, gradient_3, learning_rate,\n","                                                           lambda_=lambda_)\n","    \n","  match_cnt = 0\n","  sum_loss = 0 \n","\n","  # testing\n","  for i in range(9000):\n","      if i == 8999:\n","        acc = match_cnt / (i + 1)\n","        acc *= 100\n","        \n","        acc_list_for_test.append(acc)\n","        print(\"testing_acc : {:.4f}\".format(acc))\n","\n","        mean_loss = sum_loss/(i + 1)\n","        loss_list_for_test.append(mean_loss)\n","\n","      \n","      image = test_image[:, i]\n","      label = test_label[:, i]\n","\n","      image = np.expand_dims(image, axis=1) # 784 --> 784, 1\n","      label = np.expand_dims(label, axis=1) # 10 --> 10, 1\n","\n","      a_1, a_2, a_3, a_4 = network_forward(image, th1, th2, th3, bias1, bias2, bias3)\n","      match_cnt += is_same(a_4, label)\n","\n","      loss = criterion(a_4, label, th1, th2, th3, bias1, bias2, bias3, lambda_=lambda_)\n","      sum_loss += loss\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMyi6key4iyA","colab_type":"text"},"source":["- training until convergence at epoch 20\n","- theta and bias is applied to a normal distribution with mean 0 and standard deviation 1.\n","- learning rate decay is 0.1 every 5 epochs.\n","- only use numpy to implement gradient descent algorithm."]},{"cell_type":"markdown","metadata":{"id":"x_HMQe2VX-5f","colab_type":"text"},"source":["--- \n","4. plot the loss curve"]},{"cell_type":"code","metadata":{"id":"B9gxCs7f4lr3","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","ax = plt.figure().gca()\n","# ax.set_xticks(np.arange(epoch + 1))\n","plt.plot(loss_list_for_train, c='b')\n","plt.plot(loss_list_for_test, c='r')\n","plt.legend(['training loss', 'testing loss'])\n","\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4MG2BHZYEQY","colab_type":"text"},"source":["--- \n","5. plot the accuracy curve\n"]},{"cell_type":"code","metadata":{"id":"eZu5_c7_9J6D","colab_type":"code","colab":{}},"source":["ax = plt.figure().gca()\n","ax.set_xticks(np.arange(epoch + 1))\n","plt.plot(acc_list_for_train, c='b')\n","plt.plot(acc_list_for_test, c='r')\n","plt.legend(['training accuracy', 'testing accuracy'])\n","\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_GPT4D91YLxh","colab_type":"text"},"source":["---\n","6. plot the accuracy value"]},{"cell_type":"code","metadata":{"id":"SnP2kwp5YVOx","colab_type":"code","colab":{}},"source":["print(\"final training accuracy : {:.4f}%\".format(acc_list_for_train[-1]))\n","print(\"final testing accuracy : {:.4f}%\".format(acc_list_for_test[-1]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WPHBiWG5jXcr","colab_type":"text"},"source":["--- \n","7. plot classification example."]},{"cell_type":"code","metadata":{"id":"4eV5jtXcjaE7","colab_type":"code","colab":{}},"source":["pred_idx = np.array(pred_idx).squeeze()\n","pred_label = np.array(pred_label).squeeze()\n","index = pred_idx\n","\n","\n","right_index = index.astype(np.bool)\n","wrong_index = (1 - index).astype(np.bool)\n","\n","right_image = test_image[right_index]\n","right_label = pred_label[right_index]\n","\n","wrong_image = test_image[wrong_index]\n","wrong_label = pred_label[wrong_index]\n","\n","right_example_image = right_image[:10] \n","right_example_label = right_label[:10] \n","\n","print('\\ncorrectly classified testing images')\n","for i in range(10):\n","\n","    plt.subplot(2, 5, i+1)\n","    plt.title(right_example_label[i])\n","    plt.imshow(right_example_image[i, ...].reshape((size_row, size_col)), cmap='Greys', interpolation='None')\n","\n","    frame   = plt.gca()\n","    frame.axes.get_xaxis().set_visible(False)\n","    frame.axes.get_yaxis().set_visible(False)\n","\n","plt.show()\n","\n","print('\\nmisclassified testing images')\n","wrong_example_image = wrong_image[:10] \n","wrong_example_label = wrong_label[:10] \n","# print(right_example_image.shape)\n","\n","for i in range(10):\n","\n","    plt.subplot(2, 5, i+1)\n","    plt.title(wrong_example_label[i])\n","    plt.imshow(wrong_example_image[i, ...].reshape((size_row, size_col)), cmap='Greys', interpolation='None')\n","\n","    frame   = plt.gca()\n","    frame.axes.get_xaxis().set_visible(False)\n","    frame.axes.get_yaxis().set_visible(False)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ceDte3SYRr-S","colab_type":"text"},"source":["- present 10 correct, incorrect output and their label in 2x5 subfig"]}]}