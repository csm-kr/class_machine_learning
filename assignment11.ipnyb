{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"assignment11.ipnyb","provenance":[{"file_id":"1mHSTxoPNB45FSNvNcj5BmcJWia_HKXma","timestamp":1591321944527},{"file_id":"1bHNhafqHlIA3AaK0c6fAvfKFQBlNBnod","timestamp":1591081918804},{"file_id":"1Mr5v5OPayPbxpgFqqP9RAOAlAT6AeP9t","timestamp":1590465551420},{"file_id":"1ZOY7WhtpuImqOCq6xH9xb3SKjZpWsjDg","timestamp":1590042024344},{"file_id":"1lHsUCT-169HWJX_gH3AHVWMCPf8_fgp6","timestamp":1589267672437},{"file_id":"1kxLGScNJFdPu0TJjKnYjdBghuA8RpayJ","timestamp":1588227282478},{"file_id":"14sk42Vw2kXkbmmU_DRavxSB79umcYmY-","timestamp":1587433034302}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1QigOnxUepqi0Br-udvPuVkdftLeWwAYP","authorship_tag":"ABX9TyPQ+jWqTIp1j6LVprDR7uXH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Eh-H3h7uYq8L","colab_type":"text"},"source":["Multi-label classification using neural networks with a regularization.\n","---"]},{"cell_type":"markdown","metadata":{"id":"TJv1FZVQW5RH","colab_type":"text"},"source":["1. input data"]},{"cell_type":"code","metadata":{"id":"z_YmrfjLLpQL","colab_type":"code","outputId":"2d57297e-813d-4d11-e33b-f2dadb776c41","executionInfo":{"status":"ok","timestamp":1592200837331,"user_tz":-540,"elapsed":317754,"user":{"displayName":"조성민","photoUrl":"","userId":"00315417681750132645"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["import numpy as np\n","import re\n","import nltk\n","from sklearn.datasets import load_files\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","import pickle\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import train_test_split\n","\n","review_data = load_files(r\"drive/My Drive/Colab Notebooks/movie_review\")\n","X, y = review_data.data, review_data.target\n","\n","documents = []\n","\n","stemmer = WordNetLemmatizer()\n","\n","for sen in range(0, len(X)):\n","    # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', str(X[sen]))\n","    \n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","    \n","    # Remove single characters from the start\n","    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n","    \n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    \n","    # Removing prefixed 'b'\n","    document = re.sub(r'^b\\s+', '', document)\n","    \n","    # Converting to Lowercase\n","    document = document.lower()\n","    \n","    # Lemmatization\n","    document = document.split()\n","    document = [stemmer.lemmatize(word) for word in document]\n","    document = ' '.join(document)\n","    \n","    documents.append(document)\n","\n","vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n","X = vectorizer.fit_transform(documents).toarray()\n","\n","tfidfconverter = TfidfTransformer()\n","X = tfidfconverter.fit_transform(X).toarray()\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n","print(X_train)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","A\n"],"name":"stdout"},{"output_type":"stream","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"],"name":"stderr"},{"output_type":"stream","text":["[[0.         0.         0.         ... 0.         0.03211483 0.        ]\n"," [0.         0.         0.         ... 0.         0.08401884 0.        ]\n"," [0.         0.         0.         ... 0.06909913 0.         0.        ]\n"," ...\n"," [0.         0.         0.         ... 0.         0.03917949 0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.05031797 0.         ... 0.         0.03543396 0.        ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3RFDhWvEkM87","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"7171f2be-984d-40c3-c2fa-eaf451458fe2","executionInfo":{"status":"ok","timestamp":1592200982472,"user_tz":-540,"elapsed":1028,"user":{"displayName":"조성민","photoUrl":"","userId":"00315417681750132645"}}},"source":["print(X_train.shape)  #(1401, 1500)\n","print(y_train.shape)  #(1401,)\n","print(X_test.shape)   #(601, 1500)\n","print(y_test.shape)   #(601,)\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["(1401, 1500)\n","(1401,)\n","(601, 1500)\n","(601,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tjDnl1tSZ1P5","colab_type":"code","colab":{}},"source":["print(X_train.shape)  # 442, 1500\n","print(y_train.shape)  # 442\n","\n","print(X_test.shape)   # 190, 1500\n","print(y_test.shape)   # 190\n","\n","y_train\n","y_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPDm2rL0I6GP","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","file_data   = \"mnist.csv\"\n","handle_file = open(file_data, \"r\")\n","data        = handle_file.readlines()\n","handle_file.close()\n","\n","size_row    = 28    # height of the image\n","size_col    = 28    # width of the image\n","\n","num_image   = len(data)\n","count       = 0     # count for the number of images\n","print(num_image)    # num_image is 10000.\n","\n","\n","#\n","# normalize the values of the input data to be [0, 1]\n","#\n","def normalize(data):\n","\n","    data_normalized = (data - min(data)) / (max(data) - min(data))\n","\n","    return(data_normalized)\n","\n","#\n","# example of distance function between two vectors x and y\n","#\n","def distance(x, y):\n","\n","    d = (x - y) ** 2\n","    s = np.sum(d)\n","    # r = np.sqrt(s)\n","\n","    return(s)\n","\n","#\n","# make a matrix each column of which represents an images in a vector form\n","#\n","list_image  = np.empty((size_row * size_col, num_image), dtype=float)\n","list_label  = np.empty(num_image, dtype=int)\n","\n","for line in data:\n","\n","    line_data   = line.split(',')\n","    label       = line_data[0]\n","    im_vector   = np.asfarray(line_data[1:])\n","    im_vector   = normalize(im_vector)\n","\n","    list_label[count]       = label\n","    list_image[:, count]    = im_vector\n","\n","    count += 1\n","\n","#\n","# plot first 150 images out of 10,000 with their labels\n","#\n","f1 = plt.figure(1)\n","\n","for i in range(150):\n","\n","    label       = list_label[i]\n","    im_vector   = list_image[:, i]\n","    im_matrix   = im_vector.reshape((size_row, size_col))\n","\n","    plt.subplot(10, 15, i+1)\n","    plt.title(label)\n","    plt.imshow(im_matrix, cmap='Greys', interpolation='None')\n","\n","    frame   = plt.gca()\n","    frame.axes.get_xaxis().set_visible(False)\n","    frame.axes.get_yaxis().set_visible(False)\n","\n","\n","#plt.show()\n","\n","#\n","# plot the average image of all the images for each digit\n","#\n","f2 = plt.figure(2)\n","\n","im_average  = np.zeros((size_row * size_col, 10), dtype=float)\n","im_count    = np.zeros(10, dtype=int)\n","\n","for i in range(num_image):\n","\n","    im_average[:, list_label[i]] += list_image[:, i]\n","    im_count[list_label[i]] += 1\n","\n","for i in range(10):\n","\n","    im_average[:, i] /= im_count[i]\n","\n","    plt.subplot(2, 5, i+1)\n","    plt.title(i)\n","    plt.imshow(im_average[:,i].reshape((size_row, size_col)), cmap='Greys', interpolation='None')\n","\n","    frame   = plt.gca()\n","    frame.axes.get_xaxis().set_visible(False)\n","    frame.axes.get_yaxis().set_visible(False)\n","\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HPo20R-uSEYE","colab_type":"text"},"source":["- load the data file ('mnist.csv')\n","- each row contains label is digit $l \\in [0, 9]$ and 28 by 28 image pixel values $x \\in R^{784}$"]},{"cell_type":"code","metadata":{"id":"cv63DD8gSCT2","colab_type":"code","colab":{}},"source":["print(list_image.shape)\n","\n","# list_image = list_image.transpose()\n","# list_label = list_label.transpose()\n","\n","# # convert label_to_one_hot\n","print(list_label.shape)  # 10000, \n","num = np.unique(list_label, axis=0)\n","num = num.shape[0]\n","list_label = np.eye(num)[list_label] # \n","list_label = list_label.transpose()  # 10, 10000\n","\n","train_image = list_image[:, :1000]\n","train_label = list_label[:, :1000]\n","test_image = list_image[:, 1000:]\n","test_label = list_label[:, 1000:]\n","\n","print(train_image.shape)  # (784, 1000)\n","print(train_label.shape)  # (10, 1000)\n","\n","print(test_image.shape)   # (784, 9000)\n","print(test_label.shape)   # (10, 9000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QI8W7pt2T3hs","colab_type":"text"},"source":["- consider first 1000 images for training and rest 9000 images for testing.\n","- normalized the image data whose ranges from 0 to 1.\n","- convert label to one-hot encoding label"]},{"cell_type":"markdown","metadata":{"id":"RGKX-MJ6Y4Kj","colab_type":"text"},"source":["---\n","2. neural network architecture\n"]},{"cell_type":"code","metadata":{"id":"0qko7X-50jnh","colab_type":"code","colab":{}},"source":["def sigmoid_(z):\n","  return 1 / (1 + np.exp(-1 * z))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sm6ZUmlGnJfz","colab_type":"text"},"source":["- make sigmoid function for activation."]},{"cell_type":"code","metadata":{"id":"mgF2ZBUKYXai","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def network_forward(input_x, thetas, biases):\n","  \"\"\"\n","  input_x : a image of \n","  thetas : list of theta\n","  biases : list of bias\n","  \"\"\"\n","\n","  a = []\n","  num_layer = len(thetas)\n","  a.append(input_x)\n","  a_b = input_x                                     # a before \n","  for theta, bias in zip(thetas, biases):\n","    # print(\"theta's shape : \", theta.shape)      # 512, 784 etc...\n","    # print(\"bias' shape : \", bias.shape)         # 512, 1   etc...\n","    a_b = sigmoid_(np.matmul(theta, a_b) + bias)  # 512, 1 etc...\n","    # print(\"a_before's shape : \", a_b.shape)\n","    a.append(a_b)\n","\n","  # print(\"num of output is \", len(a))\n","  return a"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m1nUcJUqcDnE","colab_type":"text"},"source":["- make network architecture and forwarding.\n","- set parameter's type is list and return is list of output of each layers."]},{"cell_type":"code","metadata":{"id":"fHmzfZXWjQzS","colab_type":"code","colab":{}},"source":["def network_backward(a, x, y, thetas, biases, learning_rate=1e-3, lambda_=5e-4):\n","  \n","  # get the delta\n","  deltas = []\n","  delta_before = a[-1] - y                           # 10, 1\n","  deltas.append(delta_before) \n","  # ------------------------- make reverse delta --------------------------\n","  thetas_r = thetas[::-1] # 6. 5. 4. 3. 2. 1\n","  thetas_r = thetas_r[:-1]# 6, 5, 4, 3, 2\n","\n","  # a                     # 1, 2, 3, 4, 5, 6, 7\n","  a_r = a[::-1]           # 7, 6, 5, 4, 3, 2, 1  \n","  a_r = a_r[1:-1]         # 6, 5, 4, 3, 2\n","\n","  for reverse_theta, reverse_a in zip(thetas_r, a_r):\n","    delta_before = np.matmul(reverse_theta.transpose(), delta_before) * reverse_a * (1-reverse_a)\n","    deltas.append(delta_before)\n","\n","  deltas_r = deltas[::-1]  # 2, 3, 4, 5, 6, 7\n","  new_gds = []\n","\n","  for delta_r, a_ in zip(deltas_r, a):\n","    new_gds.append(delta_r @ a_.transpose())\n","\n","  new_thetas = []\n","  new_biases = []\n","\n","  for new_gd, th, delta_r, bias in zip(new_gds, thetas, deltas_r, biases):\n","    new_thetas.append(th - learning_rate * new_gd + th * lambda_)\n","    new_biases.append(bias - learning_rate * delta_r)\n","\n","  return new_thetas, new_biases"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"48s40L-UvLbw","colab_type":"text"},"source":["- make back propagation for nn by myself.\n","- if l is final layer, $\\delta^{l} = a^{l} - y$, else l is not final layer, $\\delta^{l} = (\\theta^{l-1})^{T}\\delta^{l-1} * a^{l-1} * (1 - a^{l-1})$\n","- $new gradient^{(l + 1)} = gradient^{(l)} + \\delta_{j}^{(l+1)} * (a_{j}^{(l)})^{T}$"]},{"cell_type":"code","metadata":{"id":"Wzw8SIxy1NFl","colab_type":"code","colab":{}},"source":["def criterion(out, y, thetas, biases, lambda_=5e-4):\n","  # out - shape : [10, 1]\n","  # y - shape : [10, 1] \n","  # print(\"out's shape : \", out.shape)\n","  # print(\"y's shape : \", y.shape)\n","\n","  out = out.squeeze()  # [10]\n","  y = y.squeeze()      # [10]\n","  \n","  l2_term = 0 \n","  for theta, bias in zip(thetas, biases):\n","    l2_term += np.mean(np.square(theta))\n","    l2_term += np.mean(np.square(bias))\n","\n","  if l2_term == 0:\n","    loss = np.mean(-1 * (y * np.log(out) + (1-y) * np.log(1-out)))\n","    return loss\n","\n","  l2_term *= lambda_\n","  l2_term /= 2\n","\n","  loss = np.mean(-1 * (y * np.log(out) + (1-y) * np.log(1-out))) + l2_term\n","  return loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzphviVnO23t","colab_type":"text"},"source":["- make binary cross entropy loss for multi-label classification.\n","- add l2 normalization term."]},{"cell_type":"code","metadata":{"id":"2ewJMfRcPaYe","colab_type":"code","colab":{}},"source":["def is_same(pred, gt_label):\n","  \"\"\"\n","  pred: [10, 1]\n","  gt_label : [10, 1]\n","  \"\"\"\n","  # rint(pred.shape) # 10, 1\n","  pred = np.argmax(pred, axis=0)\n","  gt_label = np.argmax(label, axis=0)\n","\n","  return int(pred==gt_label)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CEGaj9Kfqwnb","colab_type":"text"},"source":["- make a function determine that predict label and gt label is same or not."]},{"cell_type":"markdown","metadata":{"id":"pYJ075rhy4ni","colab_type":"text"},"source":["--- \n","3. training"]},{"cell_type":"code","metadata":{"id":"w8p9inSpvOWp","colab_type":"code","colab":{}},"source":["lambda_ = 5e-6\n","\n","# training\n","th1 = np.random.normal(loc=0.0, scale=0.1, size=(256, 784))\n","th2 = np.random.normal(loc=0.0, scale=0.1, size=(128, 256))\n","th3 = np.random.normal(loc=0.0, scale=0.1, size=(64, 128))\n","th4 = np.random.normal(loc=0.0, scale=0.1, size=(10, 64))\n","\n","thetas = [th1, th2, th3, th4]\n","\n","bias1 = np.random.normal(loc=0.0, scale=0.1, size=(256, 1))\n","bias2 = np.random.normal(loc=0.0, scale=0.1, size=(128, 1))\n","bias3 = np.random.normal(loc=0.0, scale=0.1, size=(64, 1))\n","bias4 = np.random.normal(loc=0.0, scale=0.1, size=(10, 1))\n","biases = [bias1, bias2, bias3, bias4]\n","\n","loss_list_for_train = []\n","acc_list_for_train = []\n","\n","loss_list_for_test = []\n","acc_list_for_test = []\n","\n","# define hyperparameter for multi-label classification\n","epochs = 30\n","learning_rate = 1e-1\n","decay_steps = [15, 30]\n","\n","for epoch in range(epochs):\n","  print('\\ntraining for epoch [{}]'.format(epoch))\n","  match_cnt = 0\n","  sum_loss = 0 \n","\n","  if epoch in decay_steps:\n","    learning_rate *= 0.5\n","\n","  # training \n","  for i in range(1000):\n","    # print(i)\n","\n","    if i == 999:\n","      acc = match_cnt / (i + 1)\n","      acc *= 100\n","      acc_list_for_train.append(acc)\n","      print(\"training_acc : {:.4f}\".format(acc))\n","\n","      mean_loss = sum_loss/(i + 1)\n","      loss_list_for_train.append(mean_loss)\n","\n","    image = train_image[:, i]\n","    label = train_label[:, i]\n","\n","    image = np.expand_dims(image, axis=1) # 784 --> 784, 1\n","    label = np.expand_dims(label, axis=1) # 10 --> 10, 1\n","    a_output = network_forward(image, thetas, biases)\n","\n","    out = a_output[-1]\n","    match_cnt += is_same(out, label)\n","    loss = criterion(out, label, thetas, biases, lambda_=lambda_)\n","\n","    sum_loss += loss \n","    thetas, biases = network_backward(a_output, image, label, thetas, biases, learning_rate, lambda_)\n","    \n","  match_cnt = 0\n","  sum_loss = 0 \n","  pred_idx = []\n","  pred_label = []\n","\n","  # testing\n","  for i in range(9000):\n","      if i == 8999:\n","        acc = match_cnt / (i + 1)\n","        acc *= 100\n","        \n","        acc_list_for_test.append(acc)\n","        print(\"testing_acc : {:.4f}\".format(acc))\n","\n","        mean_loss = sum_loss/(i + 1)\n","        loss_list_for_test.append(mean_loss)\n","\n","      \n","      image = test_image[:, i]\n","      label = test_label[:, i]\n","\n","      image = np.expand_dims(image, axis=1) # 784 --> 784, 1\n","      label = np.expand_dims(label, axis=1) # 10 --> 10, 1\n","\n","      a_output = network_forward(image, thetas, biases)\n","      out = a_output[-1]\n","      match_cnt += is_same(out, label)\n","\n","      pred = np.argmax(out, axis=0)\n","      gt_label = np.argmax(label, axis=0)\n","      pred_idx.append((pred == gt_label))\n","      pred_label.append(pred)\n","\n","      loss = criterion(out, label, thetas, biases, lambda_=lambda_)\n","      sum_loss += loss\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMyi6key4iyA","colab_type":"text"},"source":["- training until convergence at epoch 30\n","- theta and bias is applied to a normal distribution with mean 0 and standard deviation 0.1.\n","- learning rate decay is 0.5 every 15 epochs.\n","- only use numpy to implement gradient descent algorithm.\n","- network convert features like that (784 --> 256 --> 128 --> 64 --> 10)\n","- l2 norm's lambda is 5e-6."]},{"cell_type":"markdown","metadata":{"id":"x_HMQe2VX-5f","colab_type":"text"},"source":["--- \n","4. plot the loss curve"]},{"cell_type":"code","metadata":{"id":"B9gxCs7f4lr3","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","ax = plt.figure().gca()\n","# ax.set_xticks(np.arange(epoch + 1))\n","plt.plot(loss_list_for_train, c='b')\n","plt.plot(loss_list_for_test, c='r')\n","plt.legend(['training loss', 'testing loss'])\n","\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4MG2BHZYEQY","colab_type":"text"},"source":["--- \n","5. plot the accuracy curve\n"]},{"cell_type":"code","metadata":{"id":"eZu5_c7_9J6D","colab_type":"code","colab":{}},"source":["ax = plt.figure().gca()\n","ax.set_xticks(np.arange(epoch + 1))\n","plt.plot(acc_list_for_train, c='b')\n","plt.plot(acc_list_for_test, c='r')\n","plt.legend(['training accuracy', 'testing accuracy'])\n","\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_GPT4D91YLxh","colab_type":"text"},"source":["---\n","6. plot the accuracy value"]},{"cell_type":"code","metadata":{"id":"SnP2kwp5YVOx","colab_type":"code","colab":{}},"source":["print(\"final training accuracy : {:.4f}%\".format(acc_list_for_train[-1]))\n","print(\"final testing accuracy : {:.4f}%\".format(acc_list_for_test[-1]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WPHBiWG5jXcr","colab_type":"text"},"source":["--- \n","7. plot classification example."]},{"cell_type":"code","metadata":{"id":"4eV5jtXcjaE7","colab_type":"code","colab":{}},"source":["pred_idx = np.array(pred_idx).squeeze()\n","print(pred_idx)\n","pred_label = np.array(pred_label).squeeze()\n","index = pred_idx\n","\n","\n","right_index = index.astype(np.bool)\n","wrong_index = (1 - index).astype(np.bool)\n","\n","test_image_ = test_image.transpose()\n","pred_label_ = pred_label.transpose()\n","\n","right_image = test_image_[right_index]\n","right_label = pred_label_[right_index]\n","\n","right_example_image = right_image[:10] \n","right_example_label = right_label[:10] \n","\n","\n","print('\\ncorrectly classified testing images')\n","for i in range(10):\n","\n","    plt.subplot(2, 5, i+1)\n","    plt.title(right_example_label[i])\n","    plt.imshow(right_example_image[i, ...].reshape((size_row, size_col)), cmap='Greys', interpolation='None')\n","\n","    frame   = plt.gca()\n","    frame.axes.get_xaxis().set_visible(False)\n","    frame.axes.get_yaxis().set_visible(False)\n","\n","plt.show()\n","\n","print('\\nmisclassified testing images')\n","\n","wrong_image = test_image_[wrong_index]\n","wrong_label = pred_label_[wrong_index]\n","\n","wrong_example_image = wrong_image[:10] \n","wrong_example_label = wrong_label[:10] \n","# print(right_example_image.shape)\n","\n","for i in range(10):\n","\n","    plt.subplot(2, 5, i+1)\n","    plt.title(wrong_example_label[i])\n","    plt.imshow(wrong_example_image[i, ...].reshape((size_row, size_col)), cmap='Greys', interpolation='None')\n","\n","    frame   = plt.gca()\n","    frame.axes.get_xaxis().set_visible(False)\n","    frame.axes.get_yaxis().set_visible(False)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ceDte3SYRr-S","colab_type":"text"},"source":["- present 10 correct, incorrect output and their label in 2x5 subfig"]},{"cell_type":"markdown","metadata":{"id":"tyuknNLD61xO","colab_type":"text"},"source":["---\n","8. testing accuracy "]},{"cell_type":"code","metadata":{"id":"cBDLncX368iQ","colab_type":"code","colab":{}},"source":["print(\"final testing accuracy : {:.4f}%\".format(acc_list_for_test[-1]))"],"execution_count":0,"outputs":[]}]}