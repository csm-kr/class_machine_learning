{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"assignment11.ipnyb","provenance":[{"file_id":"1mHSTxoPNB45FSNvNcj5BmcJWia_HKXma","timestamp":1591321944527},{"file_id":"1bHNhafqHlIA3AaK0c6fAvfKFQBlNBnod","timestamp":1591081918804},{"file_id":"1Mr5v5OPayPbxpgFqqP9RAOAlAT6AeP9t","timestamp":1590465551420},{"file_id":"1ZOY7WhtpuImqOCq6xH9xb3SKjZpWsjDg","timestamp":1590042024344},{"file_id":"1lHsUCT-169HWJX_gH3AHVWMCPf8_fgp6","timestamp":1589267672437},{"file_id":"1kxLGScNJFdPu0TJjKnYjdBghuA8RpayJ","timestamp":1588227282478},{"file_id":"14sk42Vw2kXkbmmU_DRavxSB79umcYmY-","timestamp":1587433034302}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1QigOnxUepqi0Br-udvPuVkdftLeWwAYP","authorship_tag":"ABX9TyOT1l2FDRF20e3JkHC+Jfb/"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Eh-H3h7uYq8L","colab_type":"text"},"source":["Multi-label classification using neural networks with a regularization.\n","---"]},{"cell_type":"markdown","metadata":{"id":"TJv1FZVQW5RH","colab_type":"text"},"source":["*1*. Load text data"]},{"cell_type":"code","metadata":{"id":"z_YmrfjLLpQL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1592397616249,"user_tz":-540,"elapsed":8901,"user":{"displayName":"조성민","photoUrl":"","userId":"00315417681750132645"}},"outputId":"b79f9256-ab6d-4bf5-a977-6a432130a462"},"source":["import numpy as np\n","import re\n","import nltk\n","from sklearn.datasets import load_files\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","import pickle\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import train_test_split\n","\n","review_data = load_files(r\"drive/My Drive/Colab Notebooks/movie_review\")\n","X, y = review_data.data, review_data.target\n","\n","documents = []\n","\n","stemmer = WordNetLemmatizer()\n","\n","for sen in range(0, len(X)):\n","    # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', str(X[sen]))\n","    \n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","    \n","    # Remove single characters from the start\n","    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n","    \n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    \n","    # Removing prefixed 'b'\n","    document = re.sub(r'^b\\s+', '', document)\n","    \n","    # Converting to Lowercase\n","    document = document.lower()\n","    \n","    # Lemmatization\n","    document = document.split()\n","    document = [stemmer.lemmatize(word) for word in document]\n","    document = ' '.join(document)\n","    \n","    documents.append(document)\n","\n","vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n","X = vectorizer.fit_transform(documents).toarray()\n","\n","tfidfconverter = TfidfTransformer()\n","X = tfidfconverter.fit_transform(X).toarray()\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)"],"execution_count":181,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3RFDhWvEkM87","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1592397616250,"user_tz":-540,"elapsed":8876,"user":{"displayName":"조성민","photoUrl":"","userId":"00315417681750132645"}},"outputId":"1b4416fb-5348-4456-f20f-9deb8580ddd8"},"source":["print(X_train.shape[0])  #(1401, 1500)\n","print(y_train.shape)  #(1401,)\n","print(X_test.shape)   #(601, 1500)\n","print(y_test.shape)   #(601,)\n"],"execution_count":182,"outputs":[{"output_type":"stream","text":["1401\n","(1401,)\n","(601, 1500)\n","(601,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"13aKTkiOnmCq","colab_type":"text"},"source":["---\n","2. networks"]},{"cell_type":"code","metadata":{"id":"mqGVb6SQnoku","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592397616251,"user_tz":-540,"elapsed":8845,"user":{"displayName":"조성민","photoUrl":"","userId":"00315417681750132645"}}},"source":["def sigmoid_(z):\n","  return 1 / (1 + np.exp(-1 * z))"],"execution_count":183,"outputs":[]},{"cell_type":"code","metadata":{"id":"9f1ZHsHFn4PE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592397616252,"user_tz":-540,"elapsed":8836,"user":{"displayName":"조성민","photoUrl":"","userId":"00315417681750132645"}}},"source":["def network_forward(input_x, thetas, biases):\n","  \"\"\"\n","  input_x : a data of 1500, 1 \n","  thetas : list of theta\n","  biases : list of bias\n","  \"\"\"\n","\n","  a = []\n","  num_layer = len(thetas)\n","  a.append(input_x)\n","  a_b = input_x                                     # a before \n","  for theta, bias in zip(thetas, biases):\n","    # print(\"theta's shape : \", theta.shape)      # 512, 784 etc...\n","    # print(\"bias' shape : \", bias.shape)         # 512, 1   etc...\n","    a_b = sigmoid_(np.matmul(theta, a_b) + bias)  # 512, 1 etc...\n","    # print(\"a_before's shape : \", a_b.shape)\n","    a.append(a_b)\n","\n","  # print(\"num of output is \", len(a))\n","  return a"],"execution_count":184,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXRoC8FloPy3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592397616253,"user_tz":-540,"elapsed":8827,"user":{"displayName":"조성민","photoUrl":"","userId":"00315417681750132645"}}},"source":["def network_backward(a, x, y, thetas, biases, learning_rate=1e-3, lambda_=5e-4):\n","  \n","  # get the delta\n","  deltas = []\n","  delta_before = a[-1] - y                           # 1, 1\n","  deltas.append(delta_before) \n","  # ------------------------- make reverse delta --------------------------\n","  thetas_r = thetas[::-1] # 6. 5. 4. 3. 2. 1\n","  thetas_r = thetas_r[:-1]# 6, 5, 4, 3, 2\n","\n","  # a                     # 1, 2, 3, 4, 5, 6, 7\n","  a_r = a[::-1]           # 7, 6, 5, 4, 3, 2, 1  \n","  a_r = a_r[1:-1]         # 6, 5, 4, 3, 2\n","\n","  for reverse_theta, reverse_a in zip(thetas_r, a_r):\n","    delta_before = np.matmul(reverse_theta.transpose(), delta_before) * reverse_a * (1-reverse_a)\n","    deltas.append(delta_before)\n","\n","  deltas_r = deltas[::-1]  # 2, 3, 4, 5, 6, 7\n","  new_gds = []\n","\n","  for delta_r, a_ in zip(deltas_r, a):\n","    new_gds.append(delta_r @ a_.transpose())\n","\n","  new_thetas = []\n","  new_biases = []\n","\n","  for new_gd, th, delta_r, bias in zip(new_gds, thetas, deltas_r, biases):\n","    new_thetas.append(th - learning_rate * new_gd + th * lambda_)\n","    new_biases.append(bias - learning_rate * delta_r)\n","\n","  return new_thetas, new_biases"],"execution_count":185,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-FXwy5bsDE-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592397616253,"user_tz":-540,"elapsed":8819,"user":{"displayName":"조성민","photoUrl":"","userId":"00315417681750132645"}}},"source":["def criterion(out, y, thetas, biases, lambda_=5e-4):\n","  # out - shape : [1, 1]\n","  # y - shape : [1, 1] \n","  # print(\"out's shape : \", out.shape)\n","  # print(\"y's shape : \", y.shape)\n","\n","  out = out.squeeze(1)  # [1]\n","  y = y.squeeze(1)      # [1]\n","  \n","  l2_term = 0 \n","  for theta, bias in zip(thetas, biases):\n","    l2_term += np.mean(np.square(theta))\n","    l2_term += np.mean(np.square(bias))\n","\n","  if l2_term == 0:\n","    loss = np.mean(-1 * (y * np.log(out) + (1-y) * np.log(1-out)))\n","    return loss\n","\n","  l2_term *= lambda_\n","  l2_term /= 2\n","\n","  loss = np.mean(-1 * (y * np.log(out) + (1-y) * np.log(1-out))) + l2_term\n","  return loss"],"execution_count":186,"outputs":[]},{"cell_type":"code","metadata":{"id":"GTZIfXFuzVYM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592397616254,"user_tz":-540,"elapsed":8790,"user":{"displayName":"조성민","photoUrl":"","userId":"00315417681750132645"}}},"source":["class train_classifier(object):\n","\n","  def __init__(self, X_train, y_train, X_test, y_test):\n","\n","    # -------------------- set hyper parameters --------------------\n","    self.learning_rate = 1e-2\n","    self.lambda_ = 1e-5\n","    self.epochs = 30\n","    self.X_test = X_test\n","    self.y_test = y_test\n","\n","    # -------------------- init parameters --------------------\n","    th1 = np.random.normal(loc=0.0, scale=1, size=(256, 1500))\n","    th2 = np.random.normal(loc=0.0, scale=1, size=(128, 256))\n","    th3 = np.random.normal(loc=0.0, scale=1, size=(64, 128))\n","    th4 = np.random.normal(loc=0.0, scale=1, size=(1, 64))\n","\n","    self.thetas = [th1, th2, th3, th4]\n","\n","    bias1 = np.random.normal(loc=0.0, scale=1, size=(256, 1))\n","    bias2 = np.random.normal(loc=0.0, scale=1, size=(128, 1))\n","    bias3 = np.random.normal(loc=0.0, scale=1, size=(64, 1))\n","    bias4 = np.random.normal(loc=0.0, scale=1, size=(1, 1))\n","\n","    self.biases = [bias1, bias2, bias3, bias4] \n","\n","    self.loss_list_for_train = []\n","    self.loss_list_for_test = []\n","    self.accuracy_list_for_train = []\n","    self.accuracy_list_for_test = []\n","    self.train(X_train, y_train)\n","\n","  def train(self, X_train, y_train):\n","\n","    for epoch in range(self.epochs):\n","  \n","      print('epoch :', epoch)\n","      # if epoch == 50 or epoch == 70:\n","      #   self.learning_rate *= 0.1\n","      sum_loss = 0\n","\n","      losses = []\n","      y_list = []\n","      out_list = []\n","\n","      for idx, (x, y) in enumerate(zip(X_train, y_train)):\n","      \n","        num_data = X_train.shape[0]\n","\n","        y = np.expand_dims(y, axis=0)  # () --> (1,)\n","        # print(x.shape)  (1500, )\n","        # print(y.shape)  (1, )\n","\n","        x = np.expand_dims(x, axis=1) # 1500 --> 1500, 1\n","        y = np.expand_dims(y, axis=1) # 1 --> 1, 1\n","        a_output = network_forward(x, self.thetas, self.biases)\n","        out = a_output[-1]\n","\n","        y_list.append(y.squeeze())\n","        out_list.append(self.determine_output(out))\n","\n","        loss = criterion(out, y, self.thetas, self.biases, self.lambda_)\n","        losses.append(loss)\n","\n","        self.thetas, self.biases = network_backward(a_output, x, y, self.thetas, self.biases, \n","                                                    self.learning_rate, self.lambda_)\n","      \n","\n","      mean_loss = np.mean(losses)\n","      print(\"loss : \", mean_loss)\n","      self.loss_list_for_train.append(mean_loss)\n","      \n","      true_positive = np.equal(y_list, out_list)  \n","      acc = np.mean(true_positive)\n","      # print(\"acc : \", acc)\n","      self.accuracy_list_for_train.append(acc)\n","\n","      self.validate(self.X_test, self.y_test)\n","\n","\n","  def validate(self, X_test, Y_test):\n","\n","    losses = []\n","    y_list = []\n","    out_list = []\n","\n","    for idx, (x, y) in enumerate(zip(X_test, Y_test)):\n","\n","      y = np.expand_dims(y, axis=0)  # () --> (1,)\n","        \n","      x = np.expand_dims(x, axis=1) # 1500 --> 1500, 1\n","      y = np.expand_dims(y, axis=1) # 1 --> 1, 1\n","      a_output = network_forward(x, self.thetas, self.biases)\n","      out = a_output[-1]\n","\n","      y_list.append(y.squeeze())\n","      out_list.append(self.determine_output(out))\n","\n","      loss = criterion(out, y, self.thetas, self.biases, self.lambda_)\n","      losses.append(loss)\n","\n","    mean_loss = np.mean(losses) \n","    # print(mean_loss)\n","    self.loss_list_for_test.append(mean_loss)\n","      \n","    true_positive = np.equal(y_list, out_list)\n","    acc = np.mean(true_positive)\n","    self.accuracy_list_for_test.append(acc)\n","\n","  def determine_output(self, out):\n","\n","    out = out.squeeze()  # (1, 1)  --> ()\n","    if out >= 0.5:\n","      out = 1\n","    else:\n","      out = 0\n","\n","    # print(out)\n","    return out\n","\n","\n","  def predict(self, X):\n","\n","    predict_result = []\n","    for x_data in X:\n","      x = np.expand_dims(x_data, axis=1) # 1500 --> 1500, 1\n","      a_output = network_forward(x, self.thetas, self.biases)\n","      out = a_output[-1]\n","      predict_result.append(self.determine_output(out))\n","\n","    return predict_result\n","\n","  \n"],"execution_count":187,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5_ETF3ymu5M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"c549f81b-7b60-4fa1-fbb7-90934d00e0c5"},"source":["classifier = train_classifier(X_train, y_train, X_test, y_test)\n","y_pred_test = classifier.predict(X_test)\n","y_pred_train = classifier.predict(X_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch : 0\n","loss :  0.8257297356544259\n","epoch : 1\n","loss :  0.7183565036259759\n","epoch : 2\n","loss :  0.6933742250270359\n","epoch : 3\n","loss :  0.6742073392314731\n","epoch : 4\n","loss :  0.6576905127594815\n","epoch : 5\n","loss :  0.6430088523221328\n","epoch : 6\n","loss :  0.6287094212448149\n","epoch : 7\n","loss :  0.6140066591224728\n","epoch : 8\n","loss :  0.5983864555974452\n","epoch : 9\n","loss :  0.5815375200939056\n","epoch : 10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QWVghboa9x2t","colab_type":"code","colab":{}},"source":["from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","\n","print(confusion_matrix(y_train,y_pred_train))\n","print(classification_report(y_train,y_pred_train))\n","print(accuracy_score(y_train, y_pred_train))\n","\n","print(confusion_matrix(y_test,y_pred_test))\n","print(classification_report(y_test,y_pred_test))\n","print(accuracy_score(y_test, y_pred_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"buCPUclK_fsg","colab_type":"code","colab":{}},"source":["loss_list_for_train = classifier.loss_list_for_train\n","loss_list_for_test = classifier.loss_list_for_test\n","\n","import matplotlib.pyplot as plt\n","ax = plt.figure().gca()\n","# ax.set_xticks(np.arange(epoch + 1))\n","plt.plot(loss_list_for_train, c='b')\n","plt.plot(loss_list_for_test, c='r')\n","plt.legend(['training loss', 'Testing loss'])\n","\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ll6bBqNjS7Bi","colab_type":"code","colab":{}},"source":["acc_list_for_train = classifier.accuracy_list_for_train\n","acc_list_for_test = classifier.accuracy_list_for_test\n","\n","ax = plt.figure().gca()\n","plt.plot(acc_list_for_train, c='b')\n","plt.plot(acc_list_for_test, c='r')\n","plt.legend(['training acc', 'Testing acc'])\n","\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]}]}